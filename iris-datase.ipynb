{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iris.png](img/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Iris dataset?\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and\n",
    "biologist Ronald Fisher in his 1936 paper.<br>The data set consists of 50 samples from each of three species of Iris that is:\n",
    "- __Iris Setosa__ \n",
    "- __Iris Virginica__\n",
    "- __IrisVersicolor__\n",
    "\n",
    "Four features were measured from each sample. They are:\n",
    "- __Sepal Length__\n",
    "- __Sepal Width__\n",
    "- __Petal Length__\n",
    "- __Petal Width__\n",
    "\n",
    "All these four parameters are measured in Centimeters. Based on the combination of these four\n",
    "features, the species among three can be predicted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding logistic regression\n",
    "Despite its name, logistic regression can actually be used as a model for classification. It uses a logistic function (or sigmoid) to convert any real-valued input x into a predicted output value y^ that take values between 0 and 1, as shown in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logistic_regression.png](img/logistic_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounding y^ to the nearest integer effectively classifies the input as belonging either to class 0 or 1.\n",
    "\n",
    "Of course, most often, our problems have more than one input or feature value, x. For example, the Iris dataset provides a total of four features. For the sake of simplicity, let’s focus here on the first two features, sepal length—which we will call feature f1—and sepal width—which we will call f2. Using the tricks we learned when talking about linear regression, we know we can express the input x as a linear combination of the two features, f1 and f2:<center><br>__x=w1f1+w2f2__\n",
    "\n",
    "However, in contrast to linear regression, we are not done yet. From the previous section, we know that the sum of products would result in a real-valued, output—but we are interested in a categorical value, zero or one. This is where the logistic function comes in: it acts as a squashing function, σ, that compresses the range of possible output values to the range [0, 1]:\n",
    "<center><br>__y^=σ(x)__\n",
    "    \n",
    "Because the output is always between 0 and 1, it can be interpreted as a probability. If we only have a single input variable x, the output value y^ can be interpreted as the probability of x belonging to class 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.askaswiss.com/2017/12/how-to-classify-iris-species-using-logistic-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
